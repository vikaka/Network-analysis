for (p in 3:length(tweets_total)){
concat_tweet <- concat(concat_tweet,tweets_total[p], collapse = " ")
print(textcnt(concat_tweet,method = "string",n= 1, decreasing = T, marker = " "))
}
tweets_total[23]
tweets_total[24]
tweets_source <- VectorSource(tweets_total)
corpus <- Corpus(tweets_source)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus
corpus$`77`
corpus$77
corpus$`1`
corpus$`2`
dtm <- DocumentTermMatrix(corpus)
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
tdm2 <- as.matrix(tdm)
tdm2
frequency_2 <- colSums(tdm2)
frequency_2
frequency_2 <- sort(frequency, decreasing=TRUE)
frequency_2 <- sort(frequency, decreasing=TRUE)
frequency_2
dtm2
frequency_2 <- rowSums(tdm2)
frequency_2 <- sort(frequency, decreasing=TRUE)
frequency_2
tdm2
tdm2 <- as.data.frame(tdm)
tdm2 <- as.data.frame(tdm2)
View(tdm2)
frequency_2 <- rowSums(tdm2)
frequency_2 <- sort(frequency, decreasing=TRUE)
frequency_2
tdm2 <- as.matrix(tdm)
frequency_2 <- rowSums(tdm2)
frequency_2 <- sort(frequency_2, decreasing=TRUE)
frequency_2
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
tdm_3 <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
tdm2_3 <- as.matrix(tdm)
frequency_3 <- rowSums(tdm2)
frequency_3 <- sort(frequency_3, decreasing=TRUE)
frequency_3
tdm2_3
trigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
tdm_3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))
tdm2_3 <- as.matrix(tdm)
tdm_3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))
tdm2_3 <- as.matrix(tdm_3)
frequency_3 <- rowSums(tdm2_3)
frequency_3 <- sort(frequency_3, decreasing=TRUE)
frequency_3
frequency
frequency[1:10]
View(tweets_list)
View(tweets_list)
tweets_test<-tweets_list
tweets_test[c(1,11),]
tweets_test[,c(1,11)]
tweets_test<- tweets_test[,c(1,11)]
tweets_source <- VectorSource(tweets_test)
corpus <- Corpus(tweets_source)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
frequency
tweets_total<- tweets_list$text
library('stats')
library('networkD3')
library('igraph')
library('twitteR')
library('RCurl')
library('ngram')
library('js')
library('tau')
library('tm')
frequency_ngram1[1:10]
simpleNetwork(network_tw, zoom = "T")
frame()
View(keys)
corpus
tweets_total
frequency_2
sum(frequency_2)
sum(frequency_1)
sum(frequency_3)
sum(frequency)
frequency
View(tweets_list)
View(tweets_list)
corpus$`1`
corpus$1
corpus$`1`[1]
corpus$`1`[2]
corpus
as.character(cropus[[2]])
as.character(corpus[[2]])
View(tweets_list)
as.character(corpus[[1]])
?gsub
grep(common_1gram,tweets_list$text)
grep(common_1gram,tweets_list$text)
ommon_1gram <- frequency_ngram1[1:20]
tweets_source <- VectorSource(tweets_total)
corpus <- Corpus(tweets_source)
#After we have the corpus we do basic text processing to remove certain terms for better analysis
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
ngram1 <- DocumentTermMatrix(corpus)
ngram1_2 <- as.matrix(dtm)
frequency_ngram1 <- colSums(ngram1_2)
frequency_ngram1 <- sort(frequency_ngram1, decreasing=TRUE)
#for n= 2 ngram we use function bigram tokenizer to split words as a bigram
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
ngram2 <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
ngram2_2 <- as.matrix(ngram2)
frequency_ngram2 <- rowSums(ngram2_2)
frequency_ngram2 <- sort(frequency_ngram2, decreasing=TRUE)
#for n=3 ngram we use a funtion trigram tokenizer to split the words as a trigram
trigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
ngram3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))
ngram3_1 <- as.matrix(ngram3)
frequency_ngram3 <- rowSums(ngram3_1)
frequency_ngram3 <- sort(frequency_ngram3, decreasing=TRUE)
# now we submit the top 10 sequences for each n value
frequency_ngram1[1:10]
frequency_ngram2[1:10]
frequency_ngram3[1:10]
library('stats')
library('networkD3')
library('igraph')
library('twitteR')
library('RCurl')
library('ngram')
library('js')
library('tau')
library('tm')
tweets_source <- VectorSource(tweets_total)
corpus <- Corpus(tweets_source)
#After we have the corpus we do basic text processing to remove certain terms for better analysis
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
ngram1 <- DocumentTermMatrix(corpus)
ngram1_2 <- as.matrix(dtm)
frequency_ngram1 <- colSums(ngram1_2)
frequency_ngram1 <- sort(frequency_ngram1, decreasing=TRUE)
#for n= 2 ngram we use function bigram tokenizer to split words as a bigram
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
ngram2 <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
ngram2_2 <- as.matrix(ngram2)
frequency_ngram2 <- rowSums(ngram2_2)
frequency_ngram2 <- sort(frequency_ngram2, decreasing=TRUE)
#for n=3 ngram we use a funtion trigram tokenizer to split the words as a trigram
trigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
ngram3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))
ngram3_1 <- as.matrix(ngram3)
frequency_ngram3 <- rowSums(ngram3_1)
frequency_ngram3 <- sort(frequency_ngram3, decreasing=TRUE)
# now we submit the top 10 sequences for each n value
frequency_ngram1[1:10]
frequency_ngram2[1:10]
frequency_ngram3[1:10]
```
c) For n = 1, n = 2 and n = 3, submit the sum of all frequencies of all sequences for        that n.
```{r}
sum(frequency_ngram1)
sum(frequency_ngram2)
sum(frequency_ngram3)
common_1gram <- frequency_ngram1[1:20]
ngramnetwork <- data.frame(src = character(), target = character())
grep(common_1gram,tweets_list$text)
grep(common_1gram,tweets_list$text[1])
tweets_list$text
tweets_list$text[1]
grep(common_1gram,tweets_list$text[1])
for (i in 1:20)
{
grep(common_1gram,tweets_list$text[1])
}
warnings()
for (i in 1:20)
{
grep(common_1gram[i],tweets_list$text[1])
}
for (i in 1:20)
{
f<- grep(common_1gram[i],tweets_list$text[1])
return(f)
}
for (i in 1:20)
{
ngramnetwork <- grep(common_1gram[i],tweets_list$text[1])
}
ngramnetwork
for (i in 1:20)
{
ngramnetwork[i] <- grep(common_1gram[i],tweets_list$text[1])
}
ngramnetwork <- data.frame(src = character(), target = character())
for (i in 1:20)
{
ngramnetwork[i] <- grep(common_1gram[i],tweets_list$text[1])
}
ngramnetwork <- data.frame(src = character(), target = character())
check_f <- data.frame(stringsAsFactors=FALSE)
for (i in 1:20)
{
check_f[i] <- grep(common_1gram[i],tweets_list$text[1])
}
rm(check_f)
for (i in 1:20)
{
check_f[i] <- data.frame(grep(common_1gram[i],tweets_list$text[1])  )
}
for (i in 1:20)
{
check_f <- data.frame(grep(common_1gram[i],tweets_list$text[1])  )
}
check_f
grep(common_1gram[1],tweets_list$text[1])
grep(common_1gram[2],tweets_list$text[1])
grep(common_1gram[3],tweets_list$text[1])
a <- 0
for (i in 1:20)
{
if(grep(common_1gram[i],tweets_list$text[1]))
{a= a+1}
}
for (i in 1:20)
{
if(grep(common_1gram[i],tweets_list$text[1])>0)
{a= a+1}
}
grep(common_1gram[1],tweets_list$text[1])>0
grep(common_1gram[1],tweets_list$text[1])<1
grep(common_1gram[1],tweets_list$text[1])<4
grep(common_1gram[1],tweets_list$text[1])
grep(common_1gram[1],tweets_list$text[1]) == integer(0)
grep(common_1gram[1],tweets_list$text[1]) == integer(1)
grep(common_1gram[1],tweets_list$text[1]) == integer(2)
as.numeric(grep(common_1gram[1],tweets_list$text[1]))
as.numeric(grep(common_1gram[1],tweets_list$text[1]))+1
as.numeric(grep(common_1gram[1],tweets_list$text[1])+1)
as.numeric(grep(common_1gram[1],tweets_list$text[1],value = F)+1)
View(tweets_test)
as.numeric(grep("at",tweets_list$text[1],value = F)+1)
as.numeric(grep("intelligent future",tweets_list$text[1],value = F)+1)
as.numeric(grep(""intelligent future"",tweets_list$text[1],value = F)+1)
as.character(corpus[[2]])
grep(common_1gram[i],as.character(corpus[[2]]))
grep("see",as.character(corpus[[2]]))
commom_1gram
common_1gram
common_1gram[1]
row.names(common_1gram[1])
row.names(common_1gram)
class(common_1gram)
common_1gram
common_1gram[,1]
names(common_1gram)
grep(names(common_1gram[1]),as.character(corpus[[2]]))
names(common_1gram[1])
as.character(corpus[[2]])
grep(names(common_1gram[1]),as.character(corpus[[2]]),ignore.case = T)
length(corpus)
ngram1
corpus[[3]]
as.numeric(corpus[[3]])
names(corpus[[3]])
meta(corpus[[3]])
author(corpus[[3]])
View(tweets_list)
simpleNetwork(network_tw)
frame()
frme()
frame()
library('stringdist')
string
stringsim(as.character(corpus[[1]]),as.character(corpus[[2]]))
stringdist()
tweet_list
tweets_list
length(tweets_list)
length(tweets_list[screename])
length(tweets_list$screename)
?"stringdist"
?stringsim()
stringsim(common_1gram,as.character(corpus[[2]]))
stringsim(common_1gram[1],as.character(corpus[[2]]))
stringsim(names(common_1gram[1]),as.character(corpus[[2]]))
stringsim(names(common_1gram),as.character(corpus[[2]]))
stringsim(names(common_1gram),as.character(corpus[[2]]), method = "qgram" , q= 1)
as.data.frame(frequency)
ngramnetwork <- data.frame(src = character(), target = character(), distance = numeric())
for (i in 1:length(unique_usernames))
{
for (j in 2:length(unique_usernames))
{
ngramnetworkdata <- data.frame(src=unique_usernames[i], target=unique_usernames[j])
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
rm(i)
rm(j)
for (i in 1:100)
{
for (j in 2:100)
{
ngramnetworkdata[j] <- data.frame(src=unique_usernames[i], target=unique_usernames[i+1:100])
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
ngramnetwork <- data.frame(src = character(), target = character(), distance = numeric(), stringsAsFactors = F)
for (i in 1:100)
{
for (j in 2:100)
{
ngramnetworkdata[j] <- data.frame(src=unique_usernames[i], target=unique_usernames[i+1:100])
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
warnings()
rm(ngramnetwork)
ngramnetwork <- data.frame(src = character(), target = character(), distance = numeric(), stringsAsFactors = F)
for (i in 1:100)
{
for (j in 1:100)
{
ngramnetworkdata[j] <- data.frame(src=unique_usernames[i], target=unique_usernames[j], stringsAsFactors = F)
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
View(ngramnetwork)
for (i in 1:5)
{
for (j in 1:5)
{
ngramnetworkdata <- data.frame(src=unique_usernames[i], target=unique_usernames[j], stringsAsFactors = F)
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
View(ngramnetwork)
rm(ngramnetwork)
for (i in 1:5)
{
for (j in 1:5)
{
ngramnetworkdata <- data.frame(src=unique_usernames[i], target=unique_usernames[j], stringsAsFactors = F)
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
ngramnetwork <- data.frame(src = character(), target = character(), distance = numeric(), stringsAsFactors = F)
for (i in 1:5)
{
for (j in 1:5)
{
ngramnetworkdata <- data.frame(src=unique_usernames[i], target=unique_usernames[j], stringsAsFactors = F)
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
View(ngramnetwork)
View(ngramnetwork)
rm(ngramnetwork)
ngramnetwork <- data.frame(src = character(), target = character(), distance = numeric(), stringsAsFactors = F)
for (i in 1:100)
{
for (j in 1:100)
{
ngramnetworkdata <- data.frame(src=unique_usernames[i], target=unique_usernames[j], stringsAsFactors = F)
ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T)
}
}
View(ngramnetwork)
length(ngramnetwork)
rowlength
length(ngramnetwork$src)
library('stats')
library('networkD3')
library('igraph')
library('twitteR')
library('RCurl')
library('ngram')
library('js')
library('tau')
library('tm')
library('stringdist')
View(tweets_list)
View(tw.df)
tw.df
tw.df$screenName
tweets_freq <- data.frame(tweets = tw.df$text , rownames = tw.df$screenName, stringsAsFactors = T)
View(tweets_freq)
rm(tweets_freq)
tweets_freq <- data.frame(tweets = tw.df$text , row.names = tw.df$screenName, stringsAsFactors = T)
tweets_freq <- data.frame(screennames = tw.df$screenName,tweets = tw.df$text ,  stringsAsFactors = T)
View(tweets_freq)
tweets_freq$screennames == ngramnetwork$src[1]
tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[1]]
tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[2]]
for (k in 1:100)
{
ngramnetwork$distance[k]<-stringdist(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[i]],tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[i]])
}
View(ngramnetwork)
stringdist(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[1]],tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[1]])
tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[1]])
tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[1]]
tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[2]]
as.charachter(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[2]])
as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[2]])
for (k in 1:100)
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[i]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[i]])
}
for (k in 1:100)
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[i]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[i]])
}
for (k in 1:100)
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[i]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[i]]))
}
View(ngramnetwork)
stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[1]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[1]]))
stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[1]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[2]]))
View(ngramnetwork)
for (k in 1:100)
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]))
}
View(ngramnetworkdata)
View(ngramnetwork)
stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[1]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[3]]))
stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[1]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[4]]))
?Encoding()
tweets_freq$tweets <- Encoding(tweets_freq$tweets, "UTF-8")
tweets_freq$tweets <- iconv(tweets_freq$tweets, "UTF-8")
for (k in 1:100)
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]))
}
View(ngramnetwork)
stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[1]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[91]]))
for (k in 1:length(ngramnetwork$src))
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]))
}
View(ngramnetwork)
View(tw.df)
View(tweets_freq)
?graph.data.frame
simpleNetwork(ngramnetwork, zoom = T)
frame()
unique_usernames
View(tw.df)
tweets_freq <- data.frame(screennames = tw.df$screenName,tweets = tw.df$text ,  stringsAsFactors = F)
tweets_freq$tweets <- iconv(tweets_freq$tweets, "UTF-8")
for (k in 1:length(ngramnetwork$src))
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]))
}
warnings()
?stringdist
for (k in 1:length(ngramnetwork$src))
{
ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]), method = "qgram", q = 1)
}
View(ngramnetwork)
?simpleNetwork
library('stats')
library('networkD3')
library('igraph')
library('twitteR')
library('RCurl')
library('ngram')
library('js')
library('tau')
library('tm')
library('stringdist')
?simpleNetwork
simpleNetwork(network_tw)
frame()
