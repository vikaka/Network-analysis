---
title: "Assignment #4"
author: "Vishesh Kakarala"
date: "April 11, 2016"
output: html_document
---
```{r }
library('stats')
library('networkD3')
library('igraph')
library('twitteR')
library('RCurl')
library('ngram')
library('js')
library('tau')
library('tm')
library('stringdist')
```
#Question 1. Intro Network Analyses

a)Read in the files and visualize the network.
Read the data
```{r}
keys <- read.table("~/Assignment 4/keys.txt", sep = "\t", fill = FALSE, col.names = c("node_id","name"))
Subs <- read.table("~/Assignment 4/subs.txt", sep = "\t", fill = FALSE, col.names = c("ingredient_A","ingredient_B"))
```
Visualizing the network using forceNetwork function form networkD3 library
```{r}
keys$group <- 1

keys$node_id <- keys$node_id-1
Subs$ingredient_A <- Subs$ingredient_A-1
Subs$ingredient_B <- Subs$ingredient_B-1
forceNetwork(Links = Subs,Nodes = keys,Source = "ingredient_A", Target = "ingredient_B",NodeID = "name",Group = "group",zoom = TRUE, opacityNoHover = T)
```
b)Calculate the degree centrality of each node
```{r}
deg <- graph.data.frame(Subs,keys, directed = T)
keys$centrality <- degree(deg,mode = "Total")
keys$standardized_centrality <- keys$centrality/561

head(keys)


```
d)Which are the most “connected” node(s). 
To see which of the nodes are well connected we can check the degree centrality of the nodes, the nodes with the highest degree centrality are well connected.
```{r}
keys$name[head(order(keys$centrality,decreasing= T), n =10)]

```


c)Visually determine what are the furthest ingredients from cocoa powder. 
```{r}
keys$group[keys$name == "cocoa powder"] <- 2
# we visualize the network graph again with different group colours to identify the required node

forceNetwork(Links = Subs,Nodes = keys,Source = "ingredient_A", Target = "ingredient_B",NodeID = "name",Group = "group",zoom = TRUE, colourScale = JS("d3.scale.category10()"),opacityNoHover = T)
```
visually we can see that saffron and iceberg lettuce are the furthest away from cocoa powder


# Question 2. Crawling Twitter 

Setup account credentials
```{r}
consumer_key <- 'uV8SVavUE55eJnP8rEoWUe1Dp'
consumer_secret <- 'Y0dl77Ay9OvmjmY33xrJSO84B9r81XDp5w7jEwBHZ2CF1LJSY7'
access_token <- '718638365055303680-284qG6pZHlKwjejbDsSsjdr6nxXrWmJ'
access_secret <- 'LDaSziKwcPp0ChwMVSde0c1UCUyPQD6bQZB7gRANgCqNO'
setup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)
```
a) Download 100 users ids that have tweeted about this, and their friends/followers. Note that due to rate limits you may need to include a pause in order to be able to download data on this many users.

searching for 150 tweets with #sxsw2016
```{r}
tw.df <- do.call("rbind", lapply(searchTwitter("#SXSW2016",n =150,lang= "en"), as.data.frame))
usernames.df <- tw.df[,c("screenName")]
unique_usernames <- unique(usernames.df)

```
Now we have users sufficient for our analysis

b) Assess and plot the degree distribution of your network (choose either in-degree or out-degree and motivate why you chose the metric). 
```{r echo= F}
network_tw <- read.csv("~/Assignment 4/Twitter_network.csv", row.names=1)


```



To asses the network we need to first extract friends and followers
```{r eval= F}
network_tw <- data.frame(src=character(), target=character(), stringsAsFactors=FALSE) 

for(i in 1:100)
{
  
    start <- getUser(unique_usernames[i],retryOnRateLimit=180)
    friends.object<-lookupUsers(start$getFriendIDs(retryOnRateLimit=180))
    follower.object<-lookupUsers(start$getFollowerIDs(retryOnRateLimit=180))
    
    n<- length(friends.object)
    m<- length(follower.object)
    
    friends <- sapply(friends.object[1:n],screenName)
    followers <- sapply(follower.object[1:m],screenName)
    
    networkData <- data.frame(src=unique_usernames[i], target=friends)
    network_tw <- merge(network_tw, networkData,  all=T)
    networkData <- data.frame(src=followers, target=unique_usernames[i])
    network_tw <- merge(network_tw, networkData,  all=T)
  
    Sys.sleep(900)
  
}    
```
Degree assesment - For the purpose of analyzing popularity of each user we use in degree centrality measure
```{r}
tw_graph <- graph.data.frame(network_tw, directed = FALSE)

degree_nw <- as.data.frame(degree(tw_graph , mode = "in"))
degree_nw$node <- row.names(degree_nw)
# top 10 most connected nodes are
degree_nw[head(order(degree_nw$`degree(tw_graph, mode = "in")`,decreasing = T), n= 10),]

plot(degree.distribution(tw_graph, mode = "in"), type = "l")
```
c) Visualize the network (try using ggplot2 or networkD3 libraries).
```{r eval = F}
simpleNetwork(network_tw, zoom =T)
```
* due to the size of the network i have attached a separate file for the network visualization 


# Question 3.   Developing a Language Model (for n-gram word sequences) (10 points).    

a) Download Tweets from each user above that mention the hashtag you selected (over an appropriate time period). 

we download all tweets from each user for #SXSW2016 using the Recent mode in Searctwitter function to limit the time period

```{r echo = FALSE}
tweets_list <- read.csv("~/Assignment 4/tweetsxsw", stringsAsFactors=FALSE)
```

```{r eval= FALSE}
From <- "from:"
sxsw <- "+#SXSW2016"

tweets_list <- data.frame(stringsAsFactors=FALSE) 

for(j in 1:100)
{
  
    tweets_object<-do.call("rbind",lapply(searchTwitter(paste(From,unique_usernames[j],sxsw,sep = ""),resultType = "recent",lang= "en"), as.data.frame))
    if(length(tweets_object)>0){
    tweets_list <- merge(tweets_list, tweets_object,  all=T)
    }
}
```

```{r}
# store only the  text from the tweet in a separate object

tweets_total<- tweets_list$text


```

b) For n = 1, n = 2 and n = 3, submit the list of the 10 most frequent sequences. 
```{r}
# first we  process the ngrams for n=1, n=2 & n=3 using package tm
# here we create a corpus for analysis, in order to create a corpus we need to set up the source
tweets_source <- VectorSource(tweets_total)
corpus <- Corpus(tweets_source)
#After we have the corpus we do basic text processing to remove certain terms for better analysis
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

ngram1 <- DocumentTermMatrix(corpus)
ngram1_2 <- as.matrix(ngram1)

frequency_ngram1 <- colSums(ngram1_2)
frequency_ngram1 <- sort(frequency_ngram1, decreasing=TRUE)
#for n= 2 ngram we use function bigram tokenizer to split words as a bigram
BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

ngram2 <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
ngram2_2 <- as.matrix(ngram2)

frequency_ngram2 <- rowSums(ngram2_2)
frequency_ngram2 <- sort(frequency_ngram2, decreasing=TRUE)

#for n=3 ngram we use a funtion trigram tokenizer to split the words as a trigram

trigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)

ngram3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))
ngram3_1 <- as.matrix(ngram3)

frequency_ngram3 <- rowSums(ngram3_1)
frequency_ngram3 <- sort(frequency_ngram3, decreasing=TRUE)


# now we submit the top 10 sequences for each n value
frequency_ngram1[1:10]
frequency_ngram2[1:10]
frequency_ngram3[1:10]

```

c) For n = 1, n = 2 and n = 3, submit the sum of all frequencies of all sequences for        that n. 
```{r}
sum(frequency_ngram1)

sum(frequency_ngram2)

sum(frequency_ngram3)

```
d) Using these frequencies, generate a distance measure for individuals (e.g. they    share the X most common frequency 3-gram terms, or 2-gram terms, or 1-gram term). How does this network look compared to the network generated in  question 2? 
```{r warning= F}
# here we consolidate the usernames and tweets from the users into a data frame

tweets_freq <- data.frame(screennames = tweets_list$screenName,tweets = tweets_list$text ,  stringsAsFactors = F)

tweets_freq <- aggregate(tweets~screennames, data = tweets_freq, concat, sep = " ")

tweets_freq$tweets <- iconv(tweets_freq$tweets, "UTF-8")


# Here the dataframe containing the network information and distnace measure between the users is created

ngramnetwork <- data.frame(src = character(), target = character(), distance = numeric(), stringsAsFactors = F)

#we intitialize the network data frame with the source and target information

for (i in 1:length(tweets_freq$screennames))
{
  k = i+1
  for (j in k:length(tweets_freq$screennames))
  {
    ngramnetworkdata <- data.frame(src=tweets_freq$screennames[i], target=tweets_freq$screennames[j], stringsAsFactors = F)
    ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T) 
  
  }
  
}

# we calculate the distance measure between each user based on the string distance measured between all the tweets for the user with # SXSW with n= 3
for (k in 1:length(ngramnetwork$src))
{

  ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]), method = "qgram", q = 3)
}  
# we remove rows where there is no common frequency terms
ngramnetwork <- na.omit(ngramnetwork)  

#here we visualize th network
simpleNetwork(ngramnetwork, zoom = t)


```
From the network we can see that network based on commonly used terms is highly interconnected and denser when compared to the network graph in question 2

