{
    "contents" : "---\ntitle: \"a4part3\"\nauthor: \"Vishesh Kakarala\"\ndate: \"April 12, 2016\"\noutput: html_document\n---\n```{r}\nlibrary('stats')\nlibrary('networkD3')\nlibrary('igraph')\nlibrary('twitteR')\nlibrary('RCurl')\nlibrary('ngram')\nlibrary('js')\nlibrary('tau')\nlibrary('tm')\nlibrary('stringdist')\n```\n\n\n# Question 3.   Developing a Language Model (for n-gram word sequences) (10 points).    \n\na) Download Tweets from each user above that mention the hashtag you selected (over an appropriate time period). \n\nwe download all tweets from each user for #SXSW2016 using the Recent mode in Searctwitter function to limit the time period\n\n```{r echo = FALSE}\ntweets_list <- read.csv(\"~/Assignment 4/tweetsxsw\", stringsAsFactors=FALSE)\n```\n\n```{r eval= FALSE}\nFrom <- \"from:\"\nsxsw <- \"+#SXSW2016\"\n\ntweets_list <- data.frame(stringsAsFactors=FALSE) \n\nfor(j in 1:100)\n{\n  \n    tweets_object<-do.call(\"rbind\",lapply(searchTwitter(paste(From,unique_usernames[j],sxsw,sep = \"\"),lang= \"en\"), as.data.frame))\n    if(length(tweets_object)>0){\n    tweets_list <- merge(tweets_list, tweets_object,  all=T)\n    }\n}\n```\n\n```{r}\n# store only the  text from the tweet in a separate object\n\ntweets_total<- tweets_list$text\n\n\n```\n\nb) For n = 1, n = 2 and n = 3, submit the list of the 10 most frequent sequences. \n```{r}\n# first we  process the ngrams for n=1, n=2 & n=3 using package tm\n# here we create a corpus for analysis, in order to create a corpus we need to set up the source\ntweets_source <- VectorSource(tweets_total)\ncorpus <- Corpus(tweets_source)\n#After we have the corpus we do basic text processing to remove certain terms for better analysis\ncorpus <- tm_map(corpus, removePunctuation)\ncorpus <- tm_map(corpus, stripWhitespace)\ncorpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n\nngram1 <- DocumentTermMatrix(corpus)\nngram1_2 <- as.matrix(ngram1)\n\nfrequency_ngram1 <- colSums(ngram1_2)\nfrequency_ngram1 <- sort(frequency_ngram1, decreasing=TRUE)\n#for n= 2 ngram we use function bigram tokenizer to split words as a bigram\nBigramTokenizer <-\n  function(x)\n    unlist(lapply(ngrams(words(x), 2), paste, collapse = \" \"), use.names = FALSE)\n\nngram2 <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))\nngram2_2 <- as.matrix(ngram2)\n\nfrequency_ngram2 <- rowSums(ngram2_2)\nfrequency_ngram2 <- sort(frequency_ngram2, decreasing=TRUE)\n\n#for n=3 ngram we use a funtion trigram tokenizer to split the words as a trigram\n\ntrigramTokenizer <-\n  function(x)\n    unlist(lapply(ngrams(words(x), 3), paste, collapse = \" \"), use.names = FALSE)\n\nngram3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))\nngram3_1 <- as.matrix(ngram3)\n\nfrequency_ngram3 <- rowSums(ngram3_1)\nfrequency_ngram3 <- sort(frequency_ngram3, decreasing=TRUE)\n\n\n# now we submit the top 10 sequences for each n value\nfrequency_ngram1[1:10]\nfrequency_ngram2[1:10]\nfrequency_ngram3[1:10]\n\n```\n\nc) For n = 1, n = 2 and n = 3, submit the sum of all frequencies of all sequences for        that n. \n```{r}\nsum(frequency_ngram1)\n\nsum(frequency_ngram2)\n\nsum(frequency_ngram3)\n\n```\nd) Using these frequencies, generate a distance measure for individuals (e.g. they    share the X most common frequency 3-gram terms, or 2-gram terms, or 1-gram term). How does this network look compared to the network generated in  question 2? \n```{r warning= F}\n# here we consolidate the usernames and tweets from the users into a data frame\n\ntweets_freq <- data.frame(screennames = tweets_list$screenName,tweets = tweets_list$text ,  stringsAsFactors = F)\n\ntweets_freq <- aggregate(tweets~screennames, data = tweets_freq, concat, sep = \" \")\n\ntweets_freq$tweets <- iconv(tweets_freq$tweets, \"UTF-8\")\n\n\n# Here the dataframe containing the network information and distnace measure between the users is created\n\nngramnetwork <- data.frame(src = character(), target = character(), distance = numeric(), stringsAsFactors = F)\n\n#we intitialize the network data frame with the source and atrget information\n\nfor (i in 1:length(tweets_freq$screennames))\n{\n  k = i+1\n  for (j in k:length(tweets_freq$screennames))\n  {\n    ngramnetworkdata <- data.frame(src=tweets_freq$screennames[i], target=tweets_freq$screennames[j], stringsAsFactors = F)\n    ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T) \n  \n  }\n  \n}\n\n# we calculate the distance measure between each user based on the string distance measured between all the tweets for the user with # SXSW\nfor (k in 1:length(ngramnetwork$src))\n{\n\n  ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]), method = \"qgram\", q = 3)\n}  \n# we remove rows where there is no common frequency terms\nngramnetwork <- na.omit(ngramnetwork)  \n\n#here we visualize th network\nsimpleNetwork(ngramnetwork, zoom = t)\n```\nFrom the network we can see that network based on commonly used terms is highly interconnected which is not the case with the network graph in question 2",
    "created" : 1460439019058.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4044350021",
    "id" : "65B1690C",
    "lastKnownWriteTime" : 1460439722,
    "path" : "~/Assignment 4/a4_part3.Rmd",
    "project_path" : "a4_part3.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 11,
    "source_on_save" : false,
    "type" : "r_markdown"
}