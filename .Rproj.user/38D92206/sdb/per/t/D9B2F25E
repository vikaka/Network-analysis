{
    "contents" : "---\ntitle: \"Assignment 4\"\nauthor: \"Vishesh Kakarala\"\ndate: \"April 5, 2016\"\noutput: html_document\n---\n```{r}\nlibrary('stats')\nlibrary('networkD3')\nlibrary('igraph')\nlibrary('twitteR')\nlibrary('RCurl')\nlibrary('ngram')\nlibrary('js')\nlibrary('tau')\nlibrary('tm')\nlibrary('stringdist')\n```\n#Question 1. Intro Network Analyses\n\na) Read in the files and visualize the network.\nRead the data\n```{r}\nkeys <- read.table(\"~/Assignment 4/keys.txt\", sep = \"\\t\", fill = FALSE, col.names = c(\"node_id\",\"name\"))\nSubs <- read.table(\"~/Assignment 4/subs.txt\", sep = \"\\t\", fill = FALSE, col.names = c(\"ingredient_A\",\"ingredient_B\"))\n```\nVisualizing the network using forceNetwork function form networkD3 library\n```{r}\nkeys$group <- 1\n\nkeys$node_id <- keys$node_id-1\nSubs$ingredient_A <- Subs$ingredient_A-1\nSubs$ingredient_B <- Subs$ingredient_B-1\nforceNetwork(Links = Subs,Nodes = keys,Source = \"ingredient_A\", Target = \"ingredient_B\",NodeID = \"name\",Group = \"group\",zoom = TRUE, opacityNoHover = T)\n```\nb)Calculate the degree centrality of each node\n```{r}\ndeg <- graph.data.frame(Subs,keys, directed = T)\nkeys$centrality <- degree(deg,mode = \"Total\")\nkeys$standardized_centrality <- keys$centrality/561\n\nhead(keys)\n\n```\nd)Which are the most “connected” node(s). \nTo see which of the nodes are well connected we can check the degree centrality of the nodes, the nodes with the highest degree centrality are well connected.\n```{r}\nkeys$name[head(order(keys$centrality,decreasing= T), n =10)]\n\n```\n\n\nc)Visually determine what are the furthest ingredients from cocoa powder. \n```{r}\nkeys$group[keys$name == \"cocoa powder\"] <- 2\n# we visualize the network graph again with different group colours to identify the required node\n\nforceNetwork(Links = Subs,Nodes = keys,Source = \"ingredient_A\", Target = \"ingredient_B\",NodeID = \"name\",Group = \"group\",zoom = TRUE, colourScale = JS(\"d3.scale.category10()\"),opacityNoHover = T)\n```\nvisually we can see that saffron and iceberg lettuce are the furthest away from cocoa powder\n\n# Question 2. Crawling Twitter \n\nSetup account credentials\n```{r}\nconsumer_key <- 'uV8SVavUE55eJnP8rEoWUe1Dp'\nconsumer_secret <- 'Y0dl77Ay9OvmjmY33xrJSO84B9r81XDp5w7jEwBHZ2CF1LJSY7'\naccess_token <- '718638365055303680-284qG6pZHlKwjejbDsSsjdr6nxXrWmJ'\naccess_secret <- 'LDaSziKwcPp0ChwMVSde0c1UCUyPQD6bQZB7gRANgCqNO'\nsetup_twitter_oauth(consumer_key , consumer_secret, access_token, access_secret)\n```\na) Download 100 users ids that have tweeted about this, and their friends/followers. Note that due to rate limits you may need to include a pause in order to be able to download data on this many users.\n\nsearching for 150 tweets with #sxsw2016\n```{r}\ntw.df <- do.call(\"rbind\", lapply(searchTwitter(\"#SXSW2016\",n =150,lang= \"en\"), as.data.frame))\nusernames.df <- tw.df[,c(\"screenName\")]\nunique_usernames <- unique(usernames.df)\n\n```\nNow we have users sufficient for our analysis\n\nb) Assess and plot the degree distribution of your network (choose either in-degree or out-degree and motivate why you chose the metric). \n```{r echo= TRUE}\nnetwork_tw <- read.csv(\"~/Assignment 4/Twitter_network.csv\", row.names=1)\n\n\n```\n\n\n\nTo asses the network we need to first extract friends and followers\n```{r eval= F}\nnetwork_tw <- data.frame(src=character(), target=character(), stringsAsFactors=FALSE) \n\nfor(i in 1:100)\n{\n  \n    start <- getUser(unique_usernames[i],retryOnRateLimit=180)\n    friends.object<-lookupUsers(start$getFriendIDs(retryOnRateLimit=180))\n    follower.object<-lookupUsers(start$getFollowerIDs(retryOnRateLimit=180))\n    \n    n<- length(friends.object)\n    m<- length(follower.object)\n    \n    friends <- sapply(friends.object[1:n],screenName)\n    followers <- sapply(follower.object[1:m],screenName)\n    \n    networkData <- data.frame(src=unique_usernames[i], target=friends)\n    network_tw <- merge(network_tw, networkData,  all=T)\n    networkData <- data.frame(src=followers, target=unique_usernames[i])\n    network_tw <- merge(network_tw, networkData,  all=T)\n  \n    Sys.sleep(900)\n  \n}    \n```\nDegree assesment - For the purpose of analyzing popularity of each user we use in degree centrality measure\n```{r}\ntw_graph <- graph.data.frame(network_tw, directed = FALSE)\n\ndegree(tw_graph , mode = \"in\")\n\nplot(degree.distribution(tw_graph, mode = \"in\"), type = \"l\")\n```\nc) Visualize the network (try using ggplot2 or networkD3 libraries).\n```{r}\nsimpleNetwork(network_tw)\n```\n\n\n# Question 3.   Developing a Language Model (for n-gram word sequences) (10 points).    \n\na) Download Tweets from each user above that mention the hashtag you selected (over an appropriate time period). \n\nwe download all tweets from each user for #SXSW2016 using the Recent mode in Searctwitter function to limit the time period\n\n```{r echo = T}\ntweets_list <- read.csv(\"~/Assignment 4/tweetsxsw\", stringsAsFactors=FALSE)\n```\n\n```{r eval= FALSE}\nFrom <- \"from:\"\nsxsw <- \"+#SXSW2016\"\n\ntweets_list <- data.frame(stringsAsFactors=FALSE) \n\nfor(j in 1:100)\n{\n  \n    tweets_object<-do.call(\"rbind\",lapply(searchTwitter(paste(From,unique_usernames[j],sxsw,sep = \"\"),lang= \"en\"), as.data.frame))\n    if(length(tweets_object)>0){\n    tweets_list <- merge(tweets_list, tweets_object,  all=T)\n    }\n}\n```\n\n```{r}\n# store only the  text from the tweet in a separate object\n\ntweets_total<- tweets_list$text\n\n\n```\n\nb) For n = 1, n = 2 and n = 3, submit the list of the 10 most frequent sequences. \n```{r}\n# first we  process the ngrams for n=1, n=2 & n=3 using package tm\n# here we create a corpus for analysis, in order to create a corpus we need to set up the source\ntweets_source <- VectorSource(tweets_total)\ncorpus <- Corpus(tweets_source)\n#After we have the corpus we do basic text processing to remove certain terms for better analysis\ncorpus <- tm_map(corpus, removePunctuation)\ncorpus <- tm_map(corpus, stripWhitespace)\ncorpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n\nngram1 <- DocumentTermMatrix(corpus)\nngram1_2 <- as.matrix(dtm)\n\nfrequency_ngram1 <- colSums(ngram1_2)\nfrequency_ngram1 <- sort(frequency_ngram1, decreasing=TRUE)\n#for n= 2 ngram we use function bigram tokenizer to split words as a bigram\nBigramTokenizer <-\n  function(x)\n    unlist(lapply(ngrams(words(x), 2), paste, collapse = \" \"), use.names = FALSE)\n\nngram2 <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))\nngram2_2 <- as.matrix(ngram2)\n\nfrequency_ngram2 <- rowSums(ngram2_2)\nfrequency_ngram2 <- sort(frequency_ngram2, decreasing=TRUE)\n\n#for n=3 ngram we use a funtion trigram tokenizer to split the words as a trigram\n\ntrigramTokenizer <-\n  function(x)\n    unlist(lapply(ngrams(words(x), 3), paste, collapse = \" \"), use.names = FALSE)\n\nngram3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigramTokenizer))\nngram3_1 <- as.matrix(ngram3)\n\nfrequency_ngram3 <- rowSums(ngram3_1)\nfrequency_ngram3 <- sort(frequency_ngram3, decreasing=TRUE)\n\n\n# now we submit the top 10 sequences for each n value\nfrequency_ngram1[1:10]\nfrequency_ngram2[1:10]\nfrequency_ngram3[1:10]\n\n```\n\nc) For n = 1, n = 2 and n = 3, submit the sum of all frequencies of all sequences for        that n. \n```{r}\nsum(frequency_ngram1)\n\nsum(frequency_ngram2)\n\nsum(frequency_ngram3)\n\n```\nd) Using these frequencies, generate a distance measure for individuals (e.g. they    share the X most common frequency 3-gram terms, or 2-gram terms, or 1-gram term). How does this network look compared to the network generated in  question 2? \n```{r}\n# here we consolidate the usernames and tweets from the users into a data frame\n\ntweets_freq <- data.frame(screennames = tweets_list$screenName,tweets = tweets_list$text ,  stringsAsFactors = F)\n\ntweets_freq <- aggregate(tweets~screennames, data = tweets_freq, concat, sep = \" \")\n\ntweets_freq$tweets <- iconv(tweets_freq$tweets, \"UTF-8\")\n\n\n# Here the dataframe containing the network information and distnace measure between the users is created\n\nngramnetwork <- data.frame(src = character(), target = character(), distance = numeric(), stringsAsFactors = F)\n\n#we intitialize the network data frame with the source and atrget information\n\nfor (i in 1:length(tweets_freq$screennames))\n{\n  k = i+1\n  for (j in k:length(tweets_freq$screennames))\n  {\n    ngramnetworkdata <- data.frame(src=tweets_freq$screennames[i], target=tweets_freq$screennames[j], stringsAsFactors = F)\n    ngramnetwork <- merge(ngramnetwork,ngramnetworkdata, all = T) \n  \n  }\n  \n}\n\n# we calculate the distance measure between each user based on the string distance measured between all the tweets for the user with # SXSW\nfor (k in 1:length(ngramnetwork$src))\n{\n\n  ngramnetwork$distance[k]<-stringdist(as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$src[k]]),as.character(tweets_freq$tweets[tweets_freq$screennames == ngramnetwork$target[k]]), method = \"qgram\", q = 1)\n}  \n# we romve rows where there is no common frequency terms\nngramnetwork <- na.omit(ngramnetwork)  \n\n#here we visualize th network\nsimpleNetwork(ngramnetwork, zoom = t)\n\n```",
    "created" : 1459902545021.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1748036715",
    "id" : "D9B2F25E",
    "lastKnownWriteTime" : 1461540406,
    "path" : "~/Assignment 4/Assignment 4.Rmd",
    "project_path" : "Assignment 4.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_markdown"
}